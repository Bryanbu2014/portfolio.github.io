<!DOCTYPE html>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="../../favicon_io/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="../../favicon_io/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="./favicon_io/favicon-16x16.png"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Pacifico&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Allura&display=swap"
      rel="stylesheet"
    />
    <link rel="manifest" href="../../favicon_io/site.webmanifest" />
    <title>Coding Projects | Generative Adversarial Network</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="../css/main.css" />

    <style>
      h2 .icon.brands.fa-github {
        margin-left: 10px; /* Increase space on the left, adjust as needed */
      }
    </style>
  </head>

  <body class="is-preload">
    <!-- Header -->
    <div id="header">
      <div class="top">
        <!-- Logo -->
        <div id="logo">
          <a href="../../index.html">
            <span class="image avatar48"
              ><img src="../../images/profile-pic.JPG" alt="Profile Picture"
            /></span>
            <h1 id="title" class="signature">Bryan Bu Wen Bin</h1>
          </a>
          <p>Artificial Intelligence Student</p>
        </div>

        <!-- Nav -->
        <nav id="nav">
          <ul>
            <li>
              <a
                ><span class="icon solid fa-file"
                  >Generative Adversarial Network</span
                ></a
              >
            </li>
          </ul>
        </nav>
      </div>

      <div class="bottom">
        <!-- Social Icons -->
        <ul class="icons">
          <li>
            <a
              href="https://github.com/Bryanbu2014"
              class="icon brands fa-github"
              title="Github"
              target="_blank"
              rel="noopener noreferrer"
              ><span class="label">Github</span></a
            >
          </li>
          <li>
            <a
              href="https://www.linkedin.com/in/bubryanwb/"
              class="icon brands fa-linkedin"
              title="LinkedIn"
              target="_blank"
              rel="noopener noreferrer"
              ><span class="label">LinkedIn</span></a
            >
          </li>
        </ul>
      </div>
    </div>

    <!-- Main -->
    <div id="main">
      <!-- Intro -->
      <!-- Coding Project -->
      <section id="codingprojects" class="six">
        <div class="container">
          <header>
            <h2>
              Generative Adversarial Network
              <a
                href="https://github.com/Bryanbu2014/seminar-ki-gan-demo"
                class="icon brands fa-github"
                title="Github"
                target="_blank"
                rel="noopener noreferrer"
                ><span class="label">Generative Adversarial Network</span></a
              >
            </h2>
          </header>
          <img
            class="general-img"
            src="../../images/generative-adversarial-network/gan-overview.png"
          />
          <br />
          First of all, thank you for visiting and reading this page. If there
          is anything mentioned wrongly in the upcoming sections, please do not
          hesitate to correct me! You can find my contact somewhere in the page.
          <br />
          You may refer to my source code
          <a
            href="https://github.com/Bryanbu2014/seminar-ki-gan-demo/blob/dev/code-demo-gan.ipynb"
            ,
            target="_blank"
            >here</a
          >.
          <br />
          <br />

          <h3><u>Introduction</u></h3>
          This page records the takeways from
          <a
            href="https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff"
            ,
            target="_blank"
            >Deep Learning with Python, 2nd Edition (Manning Publications)</a
          >, chapter 12 Generative Deep Learning, section Generative Adversarial
          Network. <br /><br />
          The overview of the workflow will be organized in this way.

          <style>
            .box-data-prep {
              max-width: 100%;
              height: auto;
              width: 100%;
              max-width: 800px;
              border: 5px solid #000;
              display: flex;
              justify-content: center;
              align-items: center;
              margin: 50px auto;
              font-weight: bold;
            }

            .highlight {
              background-color: yellow;
              color: black;
            }

            .box-generator {
              width: 300px;
              height: 100px;
              border: 5px solid #000;
              display: flex;
              justify-content: center;
              align-items: center;
              margin: 10px auto;
              font-weight: bold;
              background-color: yellow;
              margin-left: auto;
              margin-right: auto;
            }

            .before-preprocessing {
              max-width: 100%; /* Ensures the image doesn't exceed the width of its container */
              height: auto; /* Maintains the aspect ratio */
              width: 100%; /* Allows the image to resize according to its original aspect ratio */
              max-width: 330px;
            }

            .after-preprocessing {
              max-width: 100%; /* Ensures the image doesn't exceed the width of its container */
              height: auto; /* Maintains the aspect ratio */
              width: 100%; /* Allows the image to resize according to its original aspect ratio */
              max-width: 400px;
            }

            .general-img {
              max-width: 100%; /* Ensures the image doesn't exceed the width of its container */
              height: auto; /* Maintains the aspect ratio */
              width: 100%; /* Allows the image to resize according to its original aspect ratio */
            }

            .result {
              margin-top: -3%;
            }
          </style>

          <div class="box-data-prep">
            Dataset downloading and preprocessing<br />
            Generator and discriminator networks setup<br />
            GAN model setup<br />
            Training and inference
          </div>
          <h3><u>Dataset Downloading and Preprocessing</u></h3>
          <p>
            Before I began hands-on work, I connected my Google Drive to my
            Notebook to facilitate file saving and resuming. In this experiment,
            I used CelebA dataset as suggested by the book. Once the dataset is
            downloaded and unzipped, we can proceed with converting the dataset
            into a TensorFlow dataset. This allows all operations to be
            performed using Tensorflow. The Keras API provides a function called
            <a href="https://keras.io/api/data_loading/image/" , target="_blank"
              ><i
                ><span class="highlight"
                  >keras.utils.image_dataset_from_directory</span
                ></i
              ></a
            >. With this function the images are resized to 64x64 and splitted
            into batches of 32 images each. Once this step is done,
            normalization is also performed to convert RGB values from range [0,
            255] to [0, 1], ensuring better stability and performance.
            <br />
            <br />
            Below are two sample images from the dataset, with the left ones
            shown before image preprocessing and the right ones after. (Left =
            Top, Right = Bottom in case you are viewing from mobile devices)
            <br />
            <img
              class="before-preprocessing"
              src="../../images/generative-adversarial-network/before-preprocessing.png"
              ,
              alt="before-preprocessing"
            />
            <img
              class="after-preprocessing"
              src="../../images/generative-adversarial-network/after-preprocessing.png"
              ,
              alt="after-preprocessing"
            />
            <br />
            Obviously, we can notice that the image on the right is much more
            blurry than the one on the left, as it is resized to 64x64. However,
            due to restrictions on computational power, dealing with an image
            size of 64x64 didn't seem to be a bad idea.
          </p>
          <h3><u>Generator Network</u></h3>
          <p>
            The generator's role in a GAN is to create fake images that are as
            realistic as possible. It aims to fool the discriminator into
            classifying them as real. The generator starts with a random noise
            vector and slowly transforms it into a realistic image through a
            series of neural network layers.
          </p>
          <p>Below are the explanation of the generator's architecture.</p>
          <div class="box-generator">Dense</div>
          <p>
            First we have dense layer. In this layer, the goal is to increase
            the data's dimensionality to have enough information to form an
            image. This also sets up the initial structure that will be refined
            by the next few layers.
          </p>
          <div class="box-generator">Reshape</div>
          <p>
            This layer reshapes the vector from the dense layer into 3D tensor
            format, which is normally required for convolutional operations.
          </p>
          <div class="box-generator">Conv2DTranspose + LeakyReLU</div>
          <p>
            We proceed with transposed convolutional layer, or so called
            deconvolutional layer. This upsampling increases the spatial
            dimensions of the image, adding more and more details from time to
            time.<br /><br />
            Activation function for these transposed convolutional layer is
            LeakyReLU. LeakyReLU is a variation of ReLU. The reason why
            LeakyReLU is chosen is that it mitigates the dying reLU problem by
            allowing small gradients for negative input. It tries to maintain
            all the neurons active.
          </p>
          <img
            class="general-img"
            src="../../images/generative-adversarial-network/relu-vs-leakyrelu.png"
            ,
            alt="relu-vs-leakyrelu"
          />
          <p>Above are the comparison between ReLU and LeakyReLU.</p>
          <div class="box-generator">Conv2D</div>
          <p>This layer produces the final output image.</p>

          <h3><u>Discriminator Network</u></h3>
          <p>
            The discriminator's job is to classify whether the data generated
            from the generator network as either real or fake. Of course, forget
            to mention, for both networks we all start from an input layer.
          </p>
          <div class="box-generator">Conv2D + LeakyReLU</div>
          <p>
            Convolutional layers to detect features, such as edges, textures,
            and other important aspects of the images. Similar to the generator
            network, we use LeakyReLU as the activation function.
          </p>
          <div class="box-generator">Flatten</div>
          <p>
            This layer converts the 3D tensor into a 1D vector. This transition
            is necessary because the next layer, which is the dense layer
            requires a 1D input.
          </p>
          <div class="box-generator">Dropout</div>
          <p>
            Dropout layer, a regularization technique. It leaves some neurons 0,
            or in simple words, deactivates it to prevent overfitting. This
            forces the network to learn more robust features that are not
            dependent on some specific neurons.
          </p>
          <div class="box-generator">Dense</div>
          <p>
            Lastly we have dense layer with sigmoid activation function. Sigmoid
            activation function is suitable for binary classfication, which
            would be the need of our case, as it outputs values between 0 and 1,
            representing fake and real.
          </p>
          <h3><u>GAN Model Setup</u></h3>
          <p>
            In this section, we have a GAN class that extends the
            <a href="https://keras.io/api/models/model/" , target="_blank"
              ><i><span class="highlight">keras.Model</span></i></a
            >
            class. We initialize the GAN class / model with a discriminator, a
            generator, and the dimensionality of the latent space. It also
            initializes metrics to track the discriminator and decorator losses,
            as it is crucial for monitoring the training process and ensuring
            both networks are improving over time.<br /><br />
            First, we will start with discriminator training. The generator uses
            random latent vectors to generate fake images, then these images are
            combined with the real ones into a batch. The images are labelled, 0
            for the fake ones and 1 for the real ones. To avoid overfitting, we
            will have to add some noise to prevent the discriminator from
            becoming too confident in its predictions.<br /><br />
            Next, as for generator training, new set of different latent vectors
            are generated and passed through the generator to create fake
            images. The discriminator then will evaluate these generated images,
            and the generator's loss based on how well these fake images can
            fool the discriminator is calculated. <br /><br />
            Both networks' weights are constantly being updated during the
            training.
          </p>
          <h3><u>Training and Inference</u></h3>
          <p>
            During the training, Adam Optimzer is applied to both generator and
            discriminator networks. Every epoch took about 2~3 minutes with
            Google A100 GPU. <br /><br />
            I recorded the training losses of both networks from epoch 36 to
            100. <br />
            <img
              class="general-img"
              src="../../images/generative-adversarial-network/loss-graph.png"
              ,
              alt="loss-graph"
            /><br />
            From this graph, we can observe that the training of the generator
            didn't go well. Starting around the 50th epoch, the loss began
            increasing significantly, indicating that the model is performing
            poorly. However, the training of the discriminator seems to be
            heading in the right direction.<br /><br />
            Below are the images generated on certain epochs
          </p>
          <p>
            <u><span style="font-weight: bold">Epoch 1</span></u>
          </p>
          <div class="result">
            <img
              src="../../images/generative-adversarial-network/epoch-image/001/generated_img_001_1.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/001/generated_img_001_2.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/001/generated_img_001_3.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/001/generated_img_001_4.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/001/generated_img_001_5.png"
            />
          </div>
          <p>
            The very first epoch, we can see that the model is trying to
            generate some so-called "haunted" images.
          </p>
          <p>
            <u><span style="font-weight: bold">Epoch 10</span></u>
          </p>
          <div class="result">
            <img
              src="../../images/generative-adversarial-network/epoch-image/010/generated_img_010_1.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/010/generated_img_010_2.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/010/generated_img_010_3.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/010/generated_img_010_4.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/010/generated_img_010_5.png"
            />
          </div>
          <p>
            We can slowly see some faces are being generated. However the faces
            are still quite distorted.
          </p>
          <p>
            <u><span style="font-weight: bold">Epoch 33</span></u>
          </p>
          <div class="result">
            <img
              src="../../images/generative-adversarial-network/epoch-image/033/generated_img_033_1.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/033/generated_img_033_2.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/033/generated_img_033_3.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/033/generated_img_033_4.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/033/generated_img_033_5.png"
            />
          </div>
          <p>Faces are now generated properly, looks good.</p>
          <p>
            <u><span style="font-weight: bold">Epoch 50</span></u>
          </p>
          <div class="result">
            <img
              src="../../images/generative-adversarial-network/epoch-image/050/generated_img_050_1.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/050/generated_img_050_2.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/050/generated_img_050_3.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/050/generated_img_050_4.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/050/generated_img_050_5.png"
            />
          </div>
          <p>
            As the training loss went up, the quality of the generated images
            become bad instead.
          </p>
          <p>
            <u><span style="font-weight: bold">Epoch 100</span></u>
          </p>
          <div class="result">
            <img
              src="../../images/generative-adversarial-network/epoch-image/100/generated_img_100_0.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/100/generated_img_100_1.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/100/generated_img_100_2.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/100/generated_img_100_3.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/100/generated_img_100_4.png"
            />
          </div>
          <p>
            Finally we reach epoch 100, but no significant improvement is
            noticed in the generated images.
          </p>
          <p>
            <u><span style="font-weight: bold">Epoch 101</span></u>
          </p>
          <div class="result">
            <img
              src="../../images/generative-adversarial-network/epoch-image/101/generated_img_101_0.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/101/generated_img_101_1.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/101/generated_img_101_2.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/101/generated_img_101_3.png"
            />
            <img
              src="../../images/generative-adversarial-network/epoch-image/101/generated_img_101_4.png"
            />
          </div>
          <p>
            Out of curiousity, I trained for one more epoch, and there you go,
            the generated images look good to me!
          </p>

          <a
            href="#"
            onclick="window.history.back(); return false;"
            class="back-to-main"
          >
            RETURN
          </a>

          <div class="row">
            <div class="col-4 col-12-mobile"></div>
          </div>
        </div>
      </section>
    </div>

    <!-- Footer -->
    <div id="footer">
      <!-- Copyright -->
      <ul class="copyright">
        <li>&copy; Bryan Bu Wen Bin. All rights reserved.</li>
        <li>Template Design: <a href="http://html5up.net">HTML5 UP</a></li>
        <li>
          Further designed and implemented by
          <a href="../../index.html">me</a>.
        </li>
      </ul>
    </div>

    <!-- Scripts -->
    <script src="../js/jquery.min.js"></script>
    <script src="../js/jquery.scrolly.min.js"></script>
    <script src="../js/jquery.scrollex.min.js"></script>
    <script src="../js/browser.min.js"></script>
    <script src="../js/breakpoints.min.js"></script>
    <script src="../js/util.js"></script>
    <script src="../js/main.js"></script>
    <script src="../js/script.js"></script>
  </body>
</html>
